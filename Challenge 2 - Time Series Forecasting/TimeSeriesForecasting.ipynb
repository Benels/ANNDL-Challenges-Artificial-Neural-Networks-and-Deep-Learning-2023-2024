{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "RagBHYLphRni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2023-2024] AN2DL/Homework2"
      ],
      "metadata": {
        "id": "SdgvC1h4h3Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW_jzwWNhFzU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Conv1D, Cropping1D, Dropout, Dense, GlobalAveragePooling1D, Reshape, Concatenate, BatchNormalization, Activation, Add, MaxPooling1D\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', size=16)\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "LWBXeMW8h80P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix randomness and hide warnings\n"
      ],
      "metadata": {
        "id": "2m6Fg_Mff_du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = np.random.randint(0, 1000)\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "random.seed(seed)\n",
        "print(\"Random Seed:\",seed);"
      ],
      "metadata": {
        "id": "nFmKfNLPf0A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "5WVLEI8_f30w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "56crCS_ah9uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSetLoad = \"Datasets/training_data.npy\"\n",
        "periodsLoad = \"Datasets/valid_periods.npy\"\n",
        "labelsLoad = \"Datasets/categories.npy\"\n",
        "unzip = False\n",
        "\n",
        "if unzip:\n",
        "    !unzip training_dataset.zip\n",
        "\n",
        "trainingSet = np.load(trainingSetLoad, allow_pickle=True)\n",
        "periods = np.load(periodsLoad, allow_pickle=True)\n",
        "categories = np.load(labelsLoad, allow_pickle=True)"
      ],
      "metadata": {
        "id": "kdB1KgCsh6fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an array containing the time series without the padding. The len of such array in fact is equal to the number of time series provided"
      ],
      "metadata": {
        "id": "rRb4cw19iDBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noPadding_data = []\n",
        "dataShape = trainingSet.shape\n",
        "for i in range(dataShape[0]):\n",
        "  noPadding_data.append(trainingSet[i,periods[i,0]:periods[i,1]])\n",
        "\n",
        "new_data = np.array(noPadding_data)\n",
        "print(new_data.shape)"
      ],
      "metadata": {
        "id": "XyWqR76hiCtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build windows\n",
        "Here you can find the function that creates the windows given the training set, the window size, the stride and the telescope (how many steps to predict). Such function also returns an array *sources* that contains the category of each window. \\\\\n",
        "In the innermost *for* loop you can also find an *if* condition, which specifies the least amount of samples required for a window to have in order to be used"
      ],
      "metadata": {
        "id": "dHVfndCpibfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequences(df, window=200, stride=20, telescope=100):\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    source = []\n",
        "    for i in range(len(df)):\n",
        "      assert window % stride == 0\n",
        "\n",
        "      temp_df = df[i].copy()\n",
        "      temp_label = df[i].copy()\n",
        "      padding_check = len(df[i]) % window\n",
        "\n",
        "      if(padding_check != 0):\n",
        "          # Compute padding length\n",
        "          padding_len = window - len(df[i]) % window\n",
        "          padding = np.zeros((padding_len), dtype='float32')\n",
        "          temp_df = np.concatenate((padding,temp_df))\n",
        "          padding = np.zeros((padding_len), dtype='float32')\n",
        "          temp_label = np.concatenate((padding,temp_label))\n",
        "          assert len(temp_df) % window == 0\n",
        "\n",
        "      for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n",
        "          # if(not np.all(temp_df[idx:idx+window] == 0)):\n",
        "          if np.count_nonzero(temp_df[idx:idx+window]) >= (window / 2):\n",
        "            dataset.append(temp_df[idx:idx+window])\n",
        "            labels.append(temp_label[idx+window:idx+window+telescope])\n",
        "            source.append(categories[i])\n",
        "\n",
        "    dataset = np.array(dataset)\n",
        "    labels = np.array(labels)\n",
        "    source = np.array(source)\n",
        "    return dataset, labels, source"
      ],
      "metadata": {
        "id": "9oToshodis-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the set of windows, the related predictions, and the array of categories for each window"
      ],
      "metadata": {
        "id": "00-JSiz3j1L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window = 200\n",
        "stride = 10\n",
        "telescope = 18\n",
        "\n",
        "X, Y, sources = build_sequences(new_data, window, stride, telescope)\n",
        "\n",
        "X.shape, Y.shape, sources.shape\n"
      ],
      "metadata": {
        "id": "FZRyGQ0Pj0FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.1, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, shuffle=True)\n",
        "\n",
        "X_train.shape, X_val.shape, y_val.shape, y_train.shape"
      ],
      "metadata": {
        "id": "3CfjMcFcdD_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect some of the windows created"
      ],
      "metadata": {
        "id": "uAPF_48LkDjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 6\n",
        "figs, axs = plt.subplots(n_images, 1, sharex=True, figsize=(14,14))\n",
        "for i in range(n_images):\n",
        "  idx = np.random.randint(len(X_train))\n",
        "  axs[i].plot(X_train[idx])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PAAyrvJqkHsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Scalers"
      ],
      "metadata": {
        "id": "ttO6jRLbcJq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Robust Scalers to the data (has only been used for the *LSTMs concatenated* model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ntEQg677cOrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Apply robust scaling (fit only to training data to avoid bias)\n",
        "rscaler_X = RobustScaler().fit(X_train)\n",
        "rscaler_y = RobustScaler().fit(y_train)\n",
        "X_train = rscaler_X.transform(X_train)\n",
        "X_val = rscaler_X.transform(X_val)\n",
        "scalerX = rscaler_X.get_params()\n",
        "scalerY = rscaler_y.get_params()\n",
        "print(\"Scaler X\",scalerX)\n",
        "print(\"Scaler Y\",scalerY)\n",
        "y_train = rscaler_y.transform(y_train)\n",
        "y_val = rscaler_y.transform(y_val)"
      ],
      "metadata": {
        "id": "WA0ju700cNyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "9vaSlesPkQMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacked LSTMs model"
      ],
      "metadata": {
        "id": "MIr9TVMu5PR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "lstm_units = 128\n",
        "\n",
        "def build_stacked_lstm_model(input_shape, lstm_units=128, dropout_rate=0.2, forecast_length=18):\n",
        "    input_layer = tfkl.Input(shape=input_shape)\n",
        "    # first block\n",
        "    x = tfkl.Bidirectional(\n",
        "        tfkl.LSTM(units=lstm_units, return_sequences=True, kernel_regularizer=tfk.regularizers.l2(0.001)))(input_layer)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Dropout(dropout_rate)(x)\n",
        "    # second block\n",
        "    x = tfkl.LSTM(units=lstm_units // 2)(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Dropout(dropout_rate)(x)\n",
        "    x = tfkl.Dense(units=forecast_length)(x)\n",
        "\n",
        "    model = tfk.Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(), metrics=[\"mse\"])\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HV0DZhIV5O5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional LSTM"
      ],
      "metadata": {
        "id": "FhDo558PkwoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CONV_LSTM(input_shape, output_shape):\n",
        "\n",
        "    # Define the input layer with the specified shape\n",
        "    input_layer = tfkl.Input(shape=input_shape)\n",
        "\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(input_layer)\n",
        "    conv = tfkl.Conv1D(128,20, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(conv)\n",
        "    conv = tfkl.Conv1D(64,10, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(32, return_sequences=True))(conv)\n",
        "\n",
        "    # Add a final Convolution layer to match the desired output shape\n",
        "    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same')(x)\n",
        "\n",
        "    # Calculate the size to crop from the output to match the output shape\n",
        "    crop_size = output_layer.shape[1] - output_shape[0]\n",
        "\n",
        "    # Crop the output to the desired length\n",
        "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
        "\n",
        "    # Construct the model by connecting input and output layers\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM')\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(), metrics=[\"mse\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ru4UGMDKkV-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder with LSTM\n",
        "One note to add on this model is that we trained it with a *weight_decay* equal to the default learning rate *(0.001)*"
      ],
      "metadata": {
        "id": "pKoalVpPk2dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_block(prev, filters, kernel_size):\n",
        "\n",
        "  x = tfkl.UpSampling1D()(prev)\n",
        "  x = tfkl.Conv1D(filters, kernel_size, padding=\"same\")(x)\n",
        "  x = tfkl.BatchNormalization()(x)\n",
        "  x = tfkl.LeakyReLU()(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def encoder_block(prev, filters, kernel_size):\n",
        "\n",
        "  x = tfkl.Conv1D(filters, kernel_size, padding=\"same\")(prev)\n",
        "  x = tfkl.BatchNormalization()(x)\n",
        "  x = tfkl.LeakyReLU()(x)\n",
        "  x = tfkl.MaxPooling1D()(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def AUTOENCODER_LSTM(input_shape ,output_shape):\n",
        "\n",
        "    input_layer = tfkl.Input(shape=input_shape)\n",
        "\n",
        "    ## ENCODER\n",
        "    x = encoder_block(input_layer, 32, 13)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(16, return_sequences=True, dropout=0.2))(x)\n",
        "    x = encoder_block(x, 64, 8)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(32, return_sequences=True, dropout=0.2))(x)\n",
        "    x = encoder_block(x, 128, 3)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True, dropout=0.2))(x)\n",
        "    # x.output_shape = (None,25,128)\n",
        "    x = tfkl.Flatten()(x)\n",
        "    x = tfkl.Dense(9)(x)\n",
        "\n",
        "    ## DECODER\n",
        "    # The shape (25,128) is the output shape of the last Bidirectional LSTM of the encoder\n",
        "    x = tfkl.Dense(25*128)(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.LeakyReLU()(x)\n",
        "    # Rebuild the shape produced by the encoder\n",
        "    x = tfkl.Reshape((25,128))(x)\n",
        "\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True, dropout=0.2))(x)\n",
        "    x = decoder_block(x, 64, 13)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True, dropout=0.2))(x)\n",
        "    x = decoder_block(x, 32, 8)\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(32, return_sequences=True, dropout=0.2))(x)\n",
        "    x = decoder_block(x, 16, 3)\n",
        "\n",
        "    x = tfkl.GlobalAveragePooling1D()(x)\n",
        "    output_layer = tfkl.Dense(18)(x)\n",
        "\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='AUTOENCODER_LSTM')\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(weight_decay=0.001), metrics=[\"mse\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "h1lWTXNIkvh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence To Sequence with Luong Attention"
      ],
      "metadata": {
        "id": "OkGxk72VdcPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_seq2seq_attention(input_shape,n_units=128):\n",
        "\n",
        "\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    encoder_x,encoder_h,encoder_c = tfkl.LSTM(units=n_units, return_sequences=True,return_state=True)(input_layer)\n",
        "\n",
        "    decoder_in = tfkl.RepeatVector(1)(encoder_h)\n",
        "\n",
        "    x = tfkl.LSTM(units=n_units, return_sequences=True,return_state=False)(decoder_in,initial_state=[encoder_h,encoder_c])\n",
        "    decoder_x = tfkl.Bidirectional(tfkl.LSTM(units=int(n_units/2), return_sequences=True,return_state=False))(x)\n",
        "\n",
        "    attention = tfkl.Dot(axes=[2,2])([decoder_x, encoder_x])\n",
        "    attention = tfkl.Activation('softmax')(attention)\n",
        "    context = tfkl.Dot(axes=[2,1])([attention,encoder_x])\n",
        "\n",
        "    concatenated_c = tfkl.Concatenate()([context,decoder_x])\n",
        "    concatenated_c = tfkl.Flatten()(concatenated_c)\n",
        "    output_layer = tfkl.Dense(18)(concatenated_c)\n",
        "\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='s2s_Attention')\n",
        "\n",
        "    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9fwZfhXDdjha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WaveNet"
      ],
      "metadata": {
        "id": "k-bkJdTKco1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_wavenet(input_shape):\n",
        "\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    x = tfkl.Conv1D(filters=16, kernel_size=3, dilation_rate=1, padding=\"causal\", activation='relu')(input_layer)\n",
        "    x = tfkl.Conv1D(filters=32, kernel_size=3, dilation_rate=2, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=64, kernel_size=3, dilation_rate=4, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=128, kernel_size=3, dilation_rate=8, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=256, kernel_size=3, dilation_rate=16, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x1 = tfkl.MaxPool1D()(x)\n",
        "\n",
        "    x1,h1,c1 = tfkl.LSTM(units=256,return_sequences=True, return_state=True)(x1)\n",
        "\n",
        "    x = tfkl.Conv1D(filters=16, kernel_size=3, dilation_rate=1, padding=\"causal\", activation='relu')(x1)\n",
        "    x = tfkl.Conv1D(filters=32, kernel_size=3, dilation_rate=2, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=64, kernel_size=3, dilation_rate=4, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=128, kernel_size=3, dilation_rate=8, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=256, kernel_size=3, dilation_rate=16, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Add()([x1, x])\n",
        "    x1 = tfkl.MaxPool1D()(x)\n",
        "\n",
        "    x1,h1,c1 = tfkl.LSTM(units=256, return_sequences=True, return_state=True)(x1, initial_state=[h1, c1])\n",
        "\n",
        "    x = tfkl.Conv1D(filters=16, kernel_size=3, dilation_rate=1, padding=\"causal\", activation='relu')(x1)\n",
        "    x = tfkl.Conv1D(filters=32, kernel_size=3, dilation_rate=2, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=64, kernel_size=3, dilation_rate=4, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=128, kernel_size=3, dilation_rate=8, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.Conv1D(filters=256, kernel_size=3, dilation_rate=16, padding=\"causal\", activation='relu')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Add()([x1, x])\n",
        "    x1 = tfkl.MaxPool1D()(x)\n",
        "\n",
        "    x1,h1,c1 = tfkl.LSTM(units=256, return_sequences=False, return_state=True)(x1, initial_state=[h1, c1])\n",
        "\n",
        "    x = tfkl.Flatten()(x1)\n",
        "    x = tfkl.Dense(256, activation='relu')(x)\n",
        "    x = tfkl.Dropout(.2)(x)\n",
        "    output_layer = tfkl.Dense(18)(x)\n",
        "\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "n6yVVIAScs6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet with LSTM"
      ],
      "metadata": {
        "id": "d67xHfL5cfLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters=64, kernel_size=3, stride=1):\n",
        "      x = tfkl.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "      shortcut = x\n",
        "      x = tfkl.BatchNormalization()(x)\n",
        "      x = tfkl.Activation('relu')(x)\n",
        "      x = tfkl.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "      x = tfkl.BatchNormalization()(x)\n",
        "      x = tfkl.Activation('relu')(x)\n",
        "      x = tfkl.Add()([x, shortcut])\n",
        "      return x\n",
        "\n",
        "def build_resNet_lstm_model(input_shape, output_shape):\n",
        "\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
        "    x = tfkl.Bidirectional(LSTM(100, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
        "    x = tfkl.Conv1D(128, 3, strides=2, padding='same')(x)\n",
        "    x = tfkl.MaxPooling1D(pool_size=2, padding=\"valid\")(x)\n",
        "\n",
        "    #ResNet Block\n",
        "    x = residual_block(x,filters=64)\n",
        "    x = residual_block(x,filters=64)\n",
        "    x = residual_block(x,filters=64)\n",
        "    x = tfkl.GlobalAveragePooling1D()(x)\n",
        "    x = tfkl.Dense(18)(x)\n",
        "    x = tfkl.Reshape((18,1))(x)\n",
        "    output_layer = tfkl.Bidirectional(tfkl.LSTM(9, return_sequences=True, name='lstm'), name='bidirectional_lstm_2')(x)\n",
        "    output_layer = tfkl.Conv1D(1, 3, padding='same', name='output_layer')(output_layer)\n",
        "\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "j-V89-5pcnw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "TSTTwkqikRvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model, print the summary and plot the model\n"
      ],
      "metadata": {
        "id": "QBiU6sH5edh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.expand_dims(X_train, axis=2)\n",
        "y = np.expand_dims(y_train, axis=2)\n",
        "\n",
        "input_shape = x.shape[1:]\n",
        "output_shape = y.shape[1:]\n",
        "\n",
        "print(input_shape, output_shape)\n",
        "model = build_stacked_lstm_model(input_shape)\n",
        "model.summary()\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
      ],
      "metadata": {
        "id": "NulDstx8efTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the training"
      ],
      "metadata": {
        "id": "DwXR8ZCyetEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 150\n",
        "history = model.fit(\n",
        "    x = x,\n",
        "    y = y,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_data = (X_val, y_val),\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=8, factor=0.2, min_lr=1e-5),\n",
        "        tfk.callbacks.ModelCheckpoint(\n",
        "            filepath='weights/model-name.{epoch:02d}-{val_loss:.4f}.h5',\n",
        "            save_freq='epoch', verbose=1, monitor='val_loss',\n",
        "            save_weights_only=True,\n",
        "        )\n",
        "    ]\n",
        ").history"
      ],
      "metadata": {
        "id": "fMKMENelkWow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "0MG44VE3kTc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "# In some models the output shape is the following (BS, 9 or 18, 1) because of the use of a Conv1D as output layer\n",
        "# the following line of code is used to remove that last dimension and to translate the prediction shape to (BS, 9 or 18)\n",
        "# predictions = predictions.reshape((predictions.shape[0], -1))[:,:9]"
      ],
      "metadata": {
        "id": "9NREFfLrkW-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell there is the code to implement the prediction with an *autoregressive* approach as explained in the report: \\\\\n",
        "predict 18 steps in the future in 2 rows, 9 samples then another 9 samples (we are assuming the model used to predict has an output layer length of 9). \\\\\n",
        "\n",
        "Note that this is an example: when uploading a submission, the *model.py* contains this code below but slightly adapted to work with the *model* class (*model.predict* -> *self.model.predict* and so on)"
      ],
      "metadata": {
        "id": "mrNX0Qhnsj5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1st_predictions = model.predict(X_test, verbose=0)\n",
        "# 1st_predictions = 1st_predictions.reshape((1st_predictions.shape[0], -1))[:,:9]\n",
        "\n",
        "# Shift the input windows of 9 time steps forward and include the 9 samples we just predicted\n",
        "nextWindow = np.concatenate((X_test[:,9:], 1st_predictions), axis=1)\n",
        "\n",
        "2nd_predictions = model.predict(nextWindow, verbose=0)\n",
        "# 2st_predictions = 2st_predictions.reshape((2st_predictions.shape[0], -1))[:,:9]\n",
        "\n",
        "predictions = np.concatenate((1st_predictions, 2st_predictions), axis=1)"
      ],
      "metadata": {
        "id": "5dv9UZLysd3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View results"
      ],
      "metadata": {
        "id": "agTAK36xwlg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the MSE and MAE metrics"
      ],
      "metadata": {
        "id": "jdAECI7dwbqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n",
        "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
        "\n",
        "# Calculate and print Mean Absolute Error (MAE)\n",
        "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
      ],
      "metadata": {
        "id": "oiwZKeRTv-Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}