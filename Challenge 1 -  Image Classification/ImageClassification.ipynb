{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQMvUTefg8Uh"
      },
      "source": [
        "# Imports\n",
        "installation and imports of all the needed dependeces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:40:54.536538Z",
          "iopub.status.busy": "2023-11-14T14:40:54.535689Z",
          "iopub.status.idle": "2023-11-14T14:41:09.306053Z",
          "shell.execute_reply": "2023-11-14T14:41:09.304852Z",
          "shell.execute_reply.started": "2023-11-14T14:40:54.536465Z"
        },
        "id": "xnh417ci2E3I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install keras_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:44:17.014807Z",
          "iopub.status.busy": "2023-11-14T14:44:17.014064Z",
          "iopub.status.idle": "2023-11-14T14:44:17.019649Z",
          "shell.execute_reply": "2023-11-14T14:44:17.018825Z",
          "shell.execute_reply.started": "2023-11-14T14:44:17.014773Z"
        },
        "id": "AoaLQpvChLpb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "# for splitting training, validation and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "# mathplot lib\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.utils import class_weight as cw\n",
        "\n",
        "\n",
        "\n",
        "# contains randAugmentation, cutMix and MixUp\n",
        "from keras_cv import layers as kcvl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRrNJMlHjz_T"
      },
      "source": [
        "# Load and process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9aUybdr2rc"
      },
      "source": [
        "import of the dataset and eventual manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:44:19.597798Z",
          "iopub.status.busy": "2023-11-14T14:44:19.597047Z",
          "iopub.status.idle": "2023-11-14T14:44:25.802498Z",
          "shell.execute_reply": "2023-11-14T14:44:25.801474Z",
          "shell.execute_reply.started": "2023-11-14T14:44:19.597764Z"
        },
        "id": "0TYy-Cpa2E3J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dataToLoad = '/kaggle/input/public-data-cleaned/public_data_cleaned.npz'\n",
        "# Conditional check for unzipping and eventual conversion to categorical label\n",
        "unzip = False\n",
        "toCat = True\n",
        "\n",
        "if unzip:\n",
        "    !unzip public_data.zip\n",
        "\n",
        "data = np.load(dataToLoad, allow_pickle = True)\n",
        "X = data['data']\n",
        "y = data['labels']\n",
        "\n",
        "# shape of the dataset\n",
        "print(X.shape)\n",
        "\n",
        "# shape of the label\n",
        "print(y.shape)\n",
        "print(y)\n",
        "\n",
        "if toCat:\n",
        "  # Create labels: 0 for 'healthy', 1 for 'unhealthy'\n",
        "  label_dict = {'healthy': 0, 'unhealthy': 1}\n",
        "  numerical_labels = [label_dict[label] for label in y]\n",
        "  labels = y\n",
        "  # Convert labels to one-hot encoding format\n",
        "  y = tfk.utils.to_categorical(numerical_labels,2)\n",
        "\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHyquvXY9V_8"
      },
      "source": [
        "## Clear data - Only performed Once\n",
        "script to clear data from the fake images, we noticed that all the fake images had a non integer value, which also makes no sense because images use integers values between 0-255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l3xmMLXdDhM"
      },
      "outputs": [],
      "source": [
        "toDelete = []\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    found = False\n",
        "    for m in X[i]:\n",
        "      if found:\n",
        "        break;\n",
        "      fractional, _ = np.modf(m)\n",
        "      if np.any(fractional != 0):\n",
        "        toDelete.append(i)\n",
        "        found = True\n",
        "\n",
        "print(len(toDelete))\n",
        "print(toDelete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKj8RFuq9_8D"
      },
      "source": [
        "Once found the index to delete, we created a dataset without fake data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMbdx2M7mk-7"
      },
      "outputs": [],
      "source": [
        "X = np.delete(X, toDelete, axis=0)\n",
        "Y = np.delete(labels, toDelete, axis=0)\n",
        "\n",
        "np.savez(\"public_data_cleaned\", data=X,labels=Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erb-4fakXrF8"
      },
      "source": [
        "## Split data\n",
        "creates validation, training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-14T14:44:25.994923Z",
          "iopub.status.busy": "2023-11-14T14:44:25.994300Z",
          "iopub.status.idle": "2023-11-14T14:44:26.315765Z",
          "shell.execute_reply": "2023-11-14T14:44:26.314689Z",
          "shell.execute_reply.started": "2023-11-14T14:44:25.994890Z"
        },
        "id": "s9ryDs3fr2Xi",
        "outputId": "aec10996-0ce5-425f-97f7-595eb57f98d9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (4002, 96, 96, 3), y_train shape: (4002, 2)\n",
            "X_val shape: (501, 96, 96, 3), y_val shape: (501, 2)\n",
            "X_test shape: (501, 96, 96, 3), y_test shape: (501, 2)\n"
          ]
        }
      ],
      "source": [
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Weights - optional\n"
      ],
      "metadata": {
        "id": "g4UlFPabADey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried to implement a different weight to every output node during the training to solve the classes disparities, but the network did not performed better. To add it to the learning is only needed to **class_weight = class_weight_vect** parameter\n",
        "\n"
      ],
      "metadata": {
        "id": "y7Wrz4QYAOnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight_vect = cw.compute_class_weight('balanced',classes=('healthy','unhealthy'), y=labels)\n",
        "class_weight_vect = {i : class_weight_vect[i] for i in range(2)}\n",
        "print(class_weight_vect)"
      ],
      "metadata": {
        "id": "6p2acrjYAJFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n",
        "> Here we provide the transformations we used during the challenge to augment our dataset.\n",
        "\n",
        "When using the EfficientNetV2 model the RandAugment transformation was actually implemented as a layer, when we started using ConvNeXtBase the RandAugment layer raised some problems during the submit procedure on codalab, since we were not able to solve them we decided to use it as a pre-train transformation (the parameter did not change)"
      ],
      "metadata": {
        "id": "c8wGISCCXuyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decide what transformations to apply to the training set\n",
        "# each transformation is applied to the original training set\n",
        "cutMix = True\n",
        "mixUp = True\n",
        "randAugment = True\n",
        "\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "if cutMix:\n",
        "  cutmix = kcvl.CutMix(alpha=0.5)\n",
        "  # apply cutmix to the original training set\n",
        "  temp = cutmix({\"images\": X_train, \"labels\": y_train})\n",
        "  # insert augmented images into augmented array\n",
        "  X_augmented = np.concatenate((X_augmented, temp[\"images\"]), axis=0)\n",
        "  y_augmented = np.concatenate((y_augmented, temp[\"labels\"]), axis=0)\n",
        "if mixUp:\n",
        "  mixUp = kcvl.MixUp(0.3)\n",
        "  # apply mixup to the original training set\n",
        "  temp = mixUp({\"images\": X_train, \"labels\": y_train})\n",
        "  # insert augmented images into augmented array\n",
        "  X_augmented = np.concatenate((X_augmented, temp[\"images\"]), axis=0)\n",
        "  y_augmented = np.concatenate((y_augmented, temp[\"labels\"]), axis=0)\n",
        "if randAugment:\n",
        "  # define the rand augment layer\n",
        "  randAugment = kcvl.RandAugment(\n",
        "      value_range=(0, 255),\n",
        "      augmentations_per_image=3,\n",
        "      magnitude=0.2,\n",
        "      magnitude_stddev=0.2,\n",
        "      rate=0.5\n",
        "    )\n",
        "  # apply the rand augment layer on the original training set\n",
        "  temp = randAugment(X_train)\n",
        "  # insert augmented images into augmented array\n",
        "  X_augmented = np.concatenate((X_augmented, temp), axis=0)\n",
        "  y_augmented = np.concatenate((y_augmented, y_train), axis=0) # randAugment preserves the labels so we simply duplicate the original ones\n",
        "\n",
        "# concatenate the original training set with the augmented images, same for the labels\n",
        "X_train = np.concatenate((X_train, X_augmented), axis=0)\n",
        "y_train = np.concatenate((X_train, y_augmented), axis=0)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "6qeBvzMDVpSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vs0Oit9hQ8k"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D78LLEVs6AA5"
      },
      "source": [
        "## Transfer Learning\n",
        "due the limitation of the dataset and the complexity of creating a good FEN we decided to use transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:46:56.395124Z",
          "iopub.status.busy": "2023-11-14T14:46:56.394429Z",
          "iopub.status.idle": "2023-11-14T14:46:56.405230Z",
          "shell.execute_reply": "2023-11-14T14:46:56.404104Z",
          "shell.execute_reply.started": "2023-11-14T14:46:56.395090Z"
        },
        "id": "0pJtND4OjdOZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def buildModel(model):\n",
        "    # download the pre-trained model\n",
        "    FEN = None\n",
        "    if model == \"effnet\":\n",
        "      FEN = tf.keras.applications.EfficientNetV2S(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=(96, 96, 3),\n",
        "        pooling=\"avg\",\n",
        "        include_preprocessing=True)\n",
        "    if model == \"convnext\":\n",
        "      FEN = tf.keras.applications.ConvNeXtBase(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=(96, 96, 3),\n",
        "        pooling=\"avg\",\n",
        "        include_preprocessing=True)\n",
        "    if FEN == None: return None\n",
        "\n",
        "    #sequential data augmentation layer, applied in order to ensure more generality due the scarisity of the dataset\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "      tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      tf.keras.layers.RandomRotation(factor=0.35, fill_mode='reflect'),\n",
        "      tf.keras.layers.RandomZoom(height_factor=-0.2),\n",
        "      tf.keras.layers.RandomContrast(0.125),\n",
        "    ], name='data_augmentation')\n",
        "\n",
        "    # freeze all its weigths of the FEN\n",
        "    FEN.trainable = False\n",
        "    # Create an input layer with shape (96, 96, 3)\n",
        "    inputs = tfk.Input(shape=(96, 96, 3))\n",
        "    # preprocess input with a sequential data augmentation\n",
        "    prep= data_augmentation(inputs)\n",
        "    # Connect FEN to the input\n",
        "    x = FEN(prep)\n",
        "\n",
        "    # we used dropout and batch normalization layers\n",
        "    # in order to generalize as possible the network learning\n",
        "    x = tfkl.Dropout(0.3)(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "\n",
        "    x = tfkl.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(1e-3))(x)\n",
        "    x = tfkl.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(1e-3))(x)\n",
        "\n",
        "    x = tfkl.Dropout(0.3)(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "\n",
        "    x = tfkl.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(1e-3))(x)\n",
        "    x = tfkl.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(1e-3))(x)\n",
        "    x = tfkl.Dropout(0.3)(x)\n",
        "    x = tfkl.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(1e-3))(x)\n",
        "\n",
        "    outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    # Create a Model connecting input and output\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "    # Compile the model with Categorical Cross-Entropy loss and Nadam optimizer\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Nadam(), metrics=['accuracy'])\n",
        "\n",
        "    # Display model summary\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbM3ZQh28cIf"
      },
      "source": [
        "Training the model with the FEN learning locked, as callback we used the early stopping in order to get the best epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T12:39:00.421335Z",
          "iopub.status.busy": "2023-11-14T12:39:00.420532Z",
          "iopub.status.idle": "2023-11-14T13:37:25.138494Z",
          "shell.execute_reply": "2023-11-14T13:37:25.137723Z",
          "shell.execute_reply.started": "2023-11-14T12:39:00.421300Z"
        },
        "id": "loZ7T818jdMB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "chosenModel = \"convnext\" # 'convnext' or 'effnet'\n",
        "model = buildModel(chosenModel)\n",
        "\n",
        "patience = 30\n",
        "batch_size = 1024\n",
        "epochs = 300\n",
        "\n",
        "history = model.fit(\n",
        "    x = X_train, #the preprocessing is already included in the FEN so no operation is needed\n",
        "    y = y_train,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_data = (X_val, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=pat, restore_best_weights=True)]\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfDdruug2E3K"
      },
      "source": [
        "Gets the best epoch in order to re train the network also with the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T13:47:13.917796Z",
          "iopub.status.busy": "2023-11-14T13:47:13.916875Z",
          "iopub.status.idle": "2023-11-14T13:47:13.922929Z",
          "shell.execute_reply": "2023-11-14T13:47:13.921987Z",
          "shell.execute_reply.started": "2023-11-14T13:47:13.917761Z"
        },
        "id": "CHcuK1ie2E3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the average best epoch\n",
        "best_epoch = len(history['loss']) - pat\n",
        "print(best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhDd8gJ82E3L"
      },
      "source": [
        "Retrain with validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T13:47:23.010669Z",
          "iopub.status.busy": "2023-11-14T13:47:23.009893Z",
          "iopub.status.idle": "2023-11-14T14:31:59.359772Z",
          "shell.execute_reply": "2023-11-14T14:31:59.358833Z",
          "shell.execute_reply.started": "2023-11-14T13:47:23.010627Z"
        },
        "id": "yCSFew4q2E3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "final_model = buildModel(chosenModel)\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "history = final_model.fit(\n",
        "    x = np.concatenate((X_train, X_val), axis=0),\n",
        "    y = np.concatenate((y_train, y_val), axis=0),\n",
        "    batch_size = batch_size,\n",
        "    epochs = best_epoch,\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:32:39.360870Z",
          "iopub.status.busy": "2023-11-14T14:32:39.360493Z",
          "iopub.status.idle": "2023-11-14T14:32:40.395119Z",
          "shell.execute_reply": "2023-11-14T14:32:40.394055Z",
          "shell.execute_reply.started": "2023-11-14T14:32:39.360840Z"
        },
        "id": "hoXMhf2q_R2I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "final_model.save(\"conv13-retrained-preFt-2-14-3.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22cUkwDNyghf"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv5xlJoY-dnq"
      },
      "source": [
        "function used to unlock layer for fine-tuning the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:46:22.241856Z",
          "iopub.status.busy": "2023-11-14T14:46:22.241467Z",
          "iopub.status.idle": "2023-11-14T14:46:22.249384Z",
          "shell.execute_reply": "2023-11-14T14:46:22.248352Z",
          "shell.execute_reply.started": "2023-11-14T14:46:22.241824Z"
        },
        "id": "8k4jbsdEyJk4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def unlock_layers(ft_model,N, layer_name):\n",
        "    # Set all FEN layers as trainable\n",
        "    ft_model.get_layer(layer_name).trainable = True\n",
        "    for i, layer in enumerate(ft_model.get_layer(layer_name).layers):\n",
        "       print(i, layer.name, layer.trainable)\n",
        "\n",
        "    # Freeze first N layers, e.g., until the 133rd one\n",
        "\n",
        "    for i, layer in enumerate(ft_model.get_layer(layer_name).layers[:N]):\n",
        "      layer.trainable=False\n",
        "    for i, layer in enumerate(ft_model.get_layer(layer_name).layers):\n",
        "       print(i, layer.name, layer.trainable)\n",
        "    ft_model.summary()\n",
        "\n",
        "    return ft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:54:16.207379Z",
          "iopub.status.busy": "2023-11-14T14:54:16.206443Z",
          "iopub.status.idle": "2023-11-14T14:54:23.726617Z",
          "shell.execute_reply": "2023-11-14T14:54:23.725753Z",
          "shell.execute_reply.started": "2023-11-14T14:54:16.207346Z"
        },
        "id": "TrasXwPsF1zi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# since keras had some problems with convnex during save/load we added a custom object to save our model in .h5\n",
        "ft_model = tfk.models.load_model('/kaggle/working/conv13-retrained-preFt-2-14-3.h5',custom_objects={'LayerScale': LayerScale})\n",
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if chosenModel == \"convnext\":\n",
        "  layer_name = \"convnext_base\"\n",
        "  layer_number = 189\n",
        "else\n",
        "  layer_name = \"efficientnetv2-s\"\n",
        "  layer_number = 343\n",
        "\n",
        "ft_model = unlock_layers(ft_model, layer_number, layer_name)\n"
      ],
      "metadata": {
        "id": "3WHvUhvlhyZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:54:32.165942Z",
          "iopub.status.busy": "2023-11-14T14:54:32.165174Z",
          "iopub.status.idle": "2023-11-14T14:54:32.185046Z",
          "shell.execute_reply": "2023-11-14T14:54:32.183904Z",
          "shell.execute_reply.started": "2023-11-14T14:54:32.165907Z"
        },
        "id": "fWdPyqwRF1wk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Nadam(1e-4), metrics='accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the FEN via fine tuning"
      ],
      "metadata": {
        "id": "un_pKe31jVMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:54:34.985420Z",
          "iopub.status.busy": "2023-11-14T14:54:34.985033Z",
          "iopub.status.idle": "2023-11-14T15:40:29.200967Z",
          "shell.execute_reply": "2023-11-14T15:40:29.200043Z",
          "shell.execute_reply.started": "2023-11-14T14:54:34.985392Z"
        },
        "id": "XveYFnj2F1tz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "patience = 30\n",
        "batch_size = 1024\n",
        "epochs = 300\n",
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_data = (X_val, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=pat, restore_best_weights=True)]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T15:42:55.157240Z",
          "iopub.status.busy": "2023-11-14T15:42:55.156454Z",
          "iopub.status.idle": "2023-11-14T15:42:57.184031Z",
          "shell.execute_reply": "2023-11-14T15:42:57.182940Z",
          "shell.execute_reply.started": "2023-11-14T15:42:55.157206Z"
        },
        "id": "avP9hdR52E3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ft_model.save('conv_13_ft-14-3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrain the FEN with both the training set and validation set with the best epoch"
      ],
      "metadata": {
        "id": "jTm44_ATjflH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T15:58:47.353740Z",
          "iopub.status.busy": "2023-11-14T15:58:47.353310Z",
          "iopub.status.idle": "2023-11-14T15:59:41.822310Z",
          "shell.execute_reply": "2023-11-14T15:59:41.820549Z",
          "shell.execute_reply.started": "2023-11-14T15:58:47.353706Z"
        },
        "id": "aLuT7F1J2E3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the average best epoch\n",
        "best_epoch = len(ft_history['loss']) - patience\n",
        "\n",
        "if chosenModel == \"convnext\":\n",
        "  layer_name = \"convnext_base\"\n",
        "  layer_number = 189\n",
        "else\n",
        "  layer_name = \"efficientnetv2-s\"\n",
        "  layer_number = 343\n",
        "\n",
        "final_model_ft = tfk.models.load_model('/kaggle/working/conv13-retrained-preFt-2-14-3.h5', custom_objects={'LayerScale': LayerScale})\n",
        "final_model_ft = unlock_layers(final_model_ft, layer_number, layer_name)\n",
        "# Compile the model\n",
        "final_model_ft.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Nadam(1e-4), metrics='accuracy')\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "history_final_ft = final_model_ft.fit(\n",
        "    x = X_train_val,\n",
        "    y = y_train_val,\n",
        "    batch_size = batch_size,\n",
        "    epochs = best_epoch,\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View results\n",
        "Display the plot of the training and compare the different versions, before fine tuning and after"
      ],
      "metadata": {
        "id": "d6lo9gwgj4dP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-13T15:57:59.002245Z",
          "iopub.status.busy": "2023-11-13T15:57:59.001568Z",
          "iopub.status.idle": "2023-11-13T15:58:07.360680Z",
          "shell.execute_reply": "2023-11-13T15:58:07.359295Z",
          "shell.execute_reply.started": "2023-11-13T15:57:59.002209Z"
        },
        "id": "ji6ON_n0F1qs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot the network before and after transfer learning to confront them\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history_final_ft['loss'], alpha=.3, color='#408537', linestyle='--')\n",
        "plt.plot(history_final_ft['val_loss'], label='Fine Tuning', alpha=.8, color='#408537')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history_final_ft['accuracy'], alpha=.3, color='#408537', linestyle='--')\n",
        "plt.plot(history_final_ft['val_accuracy'], label='Fine Tuning', alpha=.8, color='#408537')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test\n",
        "Do a prediction on the test set and return the accuracy on such predictions"
      ],
      "metadata": {
        "id": "1FiFS9_qkMyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T15:47:37.164306Z",
          "iopub.status.busy": "2023-11-14T15:47:37.163566Z",
          "iopub.status.idle": "2023-11-14T15:47:47.174120Z",
          "shell.execute_reply": "2023-11-14T15:47:47.173116Z",
          "shell.execute_reply.started": "2023-11-14T15:47:37.164267Z"
        },
        "id": "df2h3LUsCAcp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_accuracy = final_model_ft.evaluate(X_test,y_test,verbose=0)[-1]\n",
        "print('Test set accuracy %.4f' % test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save\n",
        "Save the final model"
      ],
      "metadata": {
        "id": "rlCTEyNAkW0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-10T13:50:59.368298Z",
          "iopub.status.busy": "2023-11-10T13:50:59.367594Z",
          "iopub.status.idle": "2023-11-10T13:53:38.302646Z",
          "shell.execute_reply": "2023-11-10T13:53:38.301806Z",
          "shell.execute_reply.started": "2023-11-10T13:50:59.368264Z"
        },
        "id": "ih9zLQIenGSF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "final_model_ft.save('conv_ft_268_x')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKGhkGP8o4rK"
      },
      "source": [
        "# Confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T09:57:16.985365Z",
          "iopub.status.busy": "2023-11-14T09:57:16.984387Z",
          "iopub.status.idle": "2023-11-14T09:57:22.690494Z",
          "shell.execute_reply": "2023-11-14T09:57:22.689448Z",
          "shell.execute_reply.started": "2023-11-14T09:57:16.985328Z"
        },
        "id": "yJ3IG0ltpNLf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict labels for the entire test set\n",
        "predictions = final_model_ft.predict(X_test, verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T09:57:24.427382Z",
          "iopub.status.busy": "2023-11-14T09:57:24.426968Z",
          "iopub.status.idle": "2023-11-14T09:57:26.973024Z",
          "shell.execute_reply": "2023-11-14T09:57:26.972078Z",
          "shell.execute_reply.started": "2023-11-14T09:57:24.427347Z"
        },
        "id": "TYQctLAbF6i3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "\n",
        "# Display the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, xticklabels=list(('healty','unhealty')), yticklabels=list(('healty','unhealty')), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MNSPZzk_1sV"
      },
      "source": [
        "# Utilities\n",
        "Because of some bugs we faced using newer FEN as the convnex, we had to correctly define the LayerScale class in order to upload the model between two training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T14:45:43.688109Z",
          "iopub.status.busy": "2023-11-14T14:45:43.687255Z",
          "iopub.status.idle": "2023-11-14T14:45:43.695685Z",
          "shell.execute_reply": "2023-11-14T14:45:43.694656Z",
          "shell.execute_reply.started": "2023-11-14T14:45:43.688076Z"
        },
        "id": "Ih4rvPz82E3Q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import initializers\n",
        "\n",
        "class LayerScale(layers.Layer):\n",
        "    \"\"\"Layer scale module.\n",
        "\n",
        "    References:\n",
        "      - https://arxiv.org/abs/2103.17239\n",
        "\n",
        "    Args:\n",
        "      init_values (float): Initial value for layer scale. Should be within\n",
        "        [0, 1].\n",
        "      projection_dim (int): Projection dimensionality.\n",
        "\n",
        "    Returns:\n",
        "      Tensor multiplied to the scale.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, init_values, projection_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.init_values = init_values\n",
        "        self.projection_dim = projection_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=(self.projection_dim,),\n",
        "            initializer=initializers.Constant(self.init_values),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        return x * self.gamma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"init_values\": self.init_values,\n",
        "                \"projection_dim\": self.projection_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 3995766,
          "sourceId": 6956611,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30580,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}